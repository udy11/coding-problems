{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d473f7e-697b-4603-adf5-a7d990c60e67",
   "metadata": {},
   "source": [
    "Following is based on https://www.freecodecamp.org/learn/machine-learning-with-python/machine-learning-with-python-projects/rock-paper-scissors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd3bad-ab39-46a2-8c9c-aa48d72a6cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY (RPS_game.py)\n",
    "\n",
    "import random\n",
    "\n",
    "def play(player1, player2, num_games, verbose=False):\n",
    "    p1_prev_play = \"\"\n",
    "    p2_prev_play = \"\"\n",
    "    results = {\"p1\": 0, \"p2\": 0, \"tie\": 0}\n",
    "\n",
    "    for _ in range(num_games):\n",
    "        p1_play = player1(p2_prev_play)\n",
    "        p2_play = player2(p1_prev_play)\n",
    "\n",
    "        if p1_play == p2_play:\n",
    "            results[\"tie\"] += 1\n",
    "            winner = \"Tie.\"\n",
    "        elif (p1_play == \"P\" and p2_play == \"R\") or (\n",
    "                p1_play == \"R\" and p2_play == \"S\") or (p1_play == \"S\"\n",
    "                                                       and p2_play == \"P\"):\n",
    "            results[\"p1\"] += 1\n",
    "            winner = \"Player 1 wins.\"\n",
    "        elif p2_play == \"P\" and p1_play == \"R\" or p2_play == \"R\" and p1_play == \"S\" or p2_play == \"S\" and p1_play == \"P\":\n",
    "            results[\"p2\"] += 1\n",
    "            winner = \"Player 2 wins.\"\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Player 1:\", p1_play, \"| Player 2:\", p2_play)\n",
    "            print(winner)\n",
    "            print()\n",
    "\n",
    "        p1_prev_play = p1_play\n",
    "        p2_prev_play = p2_play\n",
    "\n",
    "    games_won = results['p2'] + results['p1']\n",
    "\n",
    "    if games_won == 0:\n",
    "        win_rate = 0\n",
    "    else:\n",
    "        win_rate = results['p1'] / games_won * 100\n",
    "\n",
    "    print(\"Final results:\", results)\n",
    "    print(f\"Player 1 win rate: {win_rate}%\")\n",
    "\n",
    "    return (win_rate)\n",
    "\n",
    "\n",
    "def quincy(prev_play, counter=[0]):\n",
    "\n",
    "    counter[0] += 1\n",
    "    choices = [\"R\", \"R\", \"P\", \"P\", \"S\"]\n",
    "    return choices[counter[0] % len(choices)]\n",
    "\n",
    "\n",
    "def mrugesh(prev_opponent_play, opponent_history=[]):\n",
    "    opponent_history.append(prev_opponent_play)\n",
    "    last_ten = opponent_history[-10:]\n",
    "    most_frequent = max(set(last_ten), key=last_ten.count)\n",
    "\n",
    "    if most_frequent == '':\n",
    "        most_frequent = \"S\"\n",
    "\n",
    "    ideal_response = {'P': 'S', 'R': 'P', 'S': 'R'}\n",
    "    return ideal_response[most_frequent]\n",
    "\n",
    "\n",
    "def kris(prev_opponent_play):\n",
    "    if prev_opponent_play == '':\n",
    "        prev_opponent_play = \"R\"\n",
    "    ideal_response = {'P': 'S', 'R': 'P', 'S': 'R'}\n",
    "    return ideal_response[prev_opponent_play]\n",
    "\n",
    "\n",
    "def abbey(prev_opponent_play,\n",
    "          opponent_history=[],\n",
    "          play_order=[{\n",
    "              \"RR\": 0,\n",
    "              \"RP\": 0,\n",
    "              \"RS\": 0,\n",
    "              \"PR\": 0,\n",
    "              \"PP\": 0,\n",
    "              \"PS\": 0,\n",
    "              \"SR\": 0,\n",
    "              \"SP\": 0,\n",
    "              \"SS\": 0,\n",
    "          }]):\n",
    "\n",
    "    if not prev_opponent_play:\n",
    "        prev_opponent_play = 'R'\n",
    "    opponent_history.append(prev_opponent_play)\n",
    "\n",
    "    last_two = \"\".join(opponent_history[-2:])\n",
    "    if len(last_two) == 2:\n",
    "        play_order[0][last_two] += 1\n",
    "\n",
    "    potential_plays = [\n",
    "        prev_opponent_play + \"R\",\n",
    "        prev_opponent_play + \"P\",\n",
    "        prev_opponent_play + \"S\",\n",
    "    ]\n",
    "\n",
    "    sub_order = {\n",
    "        k: play_order[0][k]\n",
    "        for k in potential_plays if k in play_order[0]\n",
    "    }\n",
    "\n",
    "    prediction = max(sub_order, key=sub_order.get)[-1:]\n",
    "\n",
    "    ideal_response = {'P': 'S', 'R': 'P', 'S': 'R'}\n",
    "    return ideal_response[prediction]\n",
    "\n",
    "\n",
    "def human(prev_opponent_play):\n",
    "    play = \"\"\n",
    "    while play not in ['R', 'P', 'S']:\n",
    "        play = input(\"[R]ock, [P]aper, [S]cissors? \")\n",
    "        print(play)\n",
    "    return play\n",
    "\n",
    "\n",
    "def random_player(prev_opponent_play):\n",
    "    return random.choice(['R', 'P', 'S'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceba752-7803-4dcb-91dc-8f6c6292852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RPS.py\n",
    "def player(prev_play, opponent_history=[]):\n",
    "    opponent_history.append(prev_play)\n",
    "\n",
    "    guess = \"R\"\n",
    "    if len(opponent_history) > 2:\n",
    "        guess = opponent_history[-2]\n",
    "\n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881438ad-db1b-4546-b927-4a84d842d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test against all players\n",
    "play(player, quincy, 1000)\n",
    "play(player, abbey, 1000)\n",
    "play(player, kris, 1000)\n",
    "play(player, mrugesh, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2346e83-4339-45ea-b9b9-1bd770ae9c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef7c304-9269-4d42-8462-71335a7d2ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Markov Model for player(), works well for quincy and mrugesh but not for abbey and kris\n",
    "def rindex(lst, value):    # python should have rindex method for lists, unitl then this is used\n",
    "    return len(lst) - operator.indexOf(reversed(lst), value) - 1\n",
    "def markov_distributions_0(opphist):\n",
    "    ss = ''.join(opphist[rindex(opphist, ''):])    # opphist is entire history of all plays since player() function is defined, luckily each new play appends '' to the opphist, that's why we only consider history from last ''\n",
    "    rps = ('R', 'P', 'S')\n",
    "    if opphist[-1] in rps:\n",
    "        pb = [0.0, 0.0, 0.0]\n",
    "        pb[rps.index(opphist[-1])] = 1.0\n",
    "        initial_distribution = tfd.Categorical(probs = np.array(pb))\n",
    "    else:\n",
    "        initial_distribution = tfd.Categorical(probs = np.ones(3) / 3.0)\n",
    "    if ('RR' in ss) or ('RP' in ss) or ('RS' in ss):    # pbR will contain tranistion probability from R to R, P and S\n",
    "        pbR = np.array([ss.count('RR'), ss.count('RP'), ss.count('RS')])\n",
    "        pbR = pbR / pbR.sum()\n",
    "    else:\n",
    "        pbR = np.ones(3) / 3.0\n",
    "    if ('PR' in ss) or ('PP' in ss) or ('PS' in ss):    # pbP will contain tranistion probability from P to R, P and S\n",
    "        pbP = np.array([ss.count('PR'), ss.count('PP'), ss.count('PS')])\n",
    "        pbP = pbP / pbP.sum()\n",
    "    else:\n",
    "        pbP = np.ones(3) / 3.0\n",
    "    if ('SR' in ss) or ('SP' in ss) or ('SS' in ss):    # pbS will contain tranistion probability from S to R, P and S\n",
    "        pbS = np.array([ss.count('SR'), ss.count('SP'), ss.count('SS')])\n",
    "        pbS = pbS / pbS.sum()\n",
    "    else:\n",
    "        pbS = np.ones(3) / 3.0\n",
    "    transition_distribution = tfd.Categorical(probs = np.array([pbR, pbP, pbS]))\n",
    "    return initial_distribution, transition_distribution\n",
    "def player(prev_play, opponent_history = []):\n",
    "    opponent_history.append(prev_play)\n",
    "    initial_distribution, transition_distribution = markov_distributions_0(opponent_history)\n",
    "    observation_distribution = tfd.Deterministic(loc = np.array([0.0, 1.0, 2.0]))\n",
    "    #observation_distribution = tfd.Categorical(probs = np.identity(3))    # ideally we should use Categorical distribution for observations, but it doesn't work because mean() is not yet available for Categorical distributions and mode() is not available at all as of tensorflow_probability version 0.20.1. however, sample() works with Categorical\n",
    "    model = tfd.HiddenMarkovModel(initial_distribution = initial_distribution, transition_distribution = transition_distribution, observation_distribution = observation_distribution, num_steps = 2)\n",
    "    rps = ('R', 'P', 'S')\n",
    "    guess = rps[(1 + int(1.499999999999 * model.mean().numpy()[1])) % 3]    # this is very dubious way but can't see anything better until Categorical observations are possible. and I \"believe\" stretching [0,2] -> [0,3] and taking int might be better than taking round on [0, 2]\n",
    "    return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3204a09f-3743-4633-a1d7-3f3ac3d074f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [FAILED APPROACH] Hidden Markov Model for player(), this was supposed to be an improvement of above but has several problems\n",
    "# basic idea of the improvement was to not count transitions agains and again but update old values based on new entry in opponent_history\n",
    "# but there are no easy ways to keep track of how many transitions occured separately for each of R, P and S\n",
    "# furthermore, there is no significant improvement observed in speed\n",
    "def rindex(lst, value):\n",
    "    return len(lst) - operator.indexOf(reversed(lst), value) - 1\n",
    "def markov_distributions(opphist, dists = [None, None]):    # this will initialize dists when function is first defined and retains latest value in any successive calls. this is dangerous method if not being careful\n",
    "    rps = ('R', 'P', 'S')\n",
    "    nhist = len(opphist[rindex(opphist, ''):]) - 1    # opphist is entire history of all plays since player() function is defined, luckily each new play appends '' to the opphist, that's why we only consider history from last ''\n",
    "    if opphist[-1] in rps:\n",
    "        nm1 = rps.index(opphist[-1])\n",
    "        pb = np.zeros(3)\n",
    "        pb[nm1] = 1.0\n",
    "        dists[0] = tfd.Categorical(probs = np.array(pb))\n",
    "    else:\n",
    "        dists[0] = tfd.Categorical(probs = np.ones(3) / 3.0)\n",
    "    if (nhist > 1) and (opphist[-1] in rps) and (opphist[-2] in rps):\n",
    "        nm0 = rps.index(opphist[-2])\n",
    "        pb = dists[1].probs.numpy()\n",
    "        pb[nm0, :] = pb[nm0, :] * (nhist - 2.0) / (nhist - 1.0)    # this is NOT correct way. instead of nhist, only number of transitions from nm0 should be taken into account\n",
    "        pb[nm0, nm1] += 1.0 / (nhist - 1.0)\n",
    "        dists[1] = tfd.Categorical(probs = pb)\n",
    "    else:\n",
    "        dists[1] = tfd.Categorical(probs = np.zeros((3, 3)))\n",
    "    d10 = dists[1].probs.numpy()[0, :]    # this needs to be done because none of the row of transition_distribution should be all 0.0\n",
    "    d11 = dists[1].probs.numpy()[1, :]\n",
    "    d12 = dists[1].probs.numpy()[2, :]\n",
    "    if all(d10 == 0.0):\n",
    "        d10 = np.ones(3) / 3.0\n",
    "    if all(d11 == 0.0):\n",
    "        d11 = np.ones(3) / 3.0\n",
    "    if all(d12 == 0.0):\n",
    "        d12 = np.ones(3) / 3.0\n",
    "    return [dists[0], tfd.Categorical(probs = np.array([d10, d11, d12]))]\n",
    "def player(prev_play, opponent_history=[]):\n",
    "    opponent_history.append(prev_play)\n",
    "    initial_distribution, transition_distribution = markov_distributions(opponent_history)\n",
    "    observation_distribution = tfd.Deterministic(loc = np.array([0.0, 1.0, 2.0]))\n",
    "    #observation_distribution = tfd.Categorical(probs = np.identity(3))    # ideally we should use Categorical distribution for observations, but it doesn't work because mean() is not yet available for Categorical distributions and mode() is not available at all as of tensorflow_probability version 0.20.1. however, sample() works with Categorical\n",
    "    model = tfd.HiddenMarkovModel(initial_distribution = initial_distribution, transition_distribution = transition_distribution, observation_distribution = observation_distribution, num_steps = 2)\n",
    "    rps = ('R', 'P', 'S')\n",
    "    guess = rps[(1 + int(1.499999999999 * model.mean().numpy()[1])) % 3]    # this is very dubious way but can't see anything better until Categorical observations are possible. and I \"believe\" stretching [0,2] -> [0,3] and taking int might be better than taking round on [0, 2]\n",
    "    return guess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9324d575-3b50-4db5-a3c5-f88ddb053bdc",
   "metadata": {},
   "source": [
    "Couldn't much understand or got to working with pomegranate and hmmlearn libraries for Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f25093-f78b-4d81-9a6c-e01dc9f8c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this approach only uses opponent's history and trains the model for the entire history again before each guess\n",
    "rps = ('R', 'P', 'S')\n",
    "rps2int = {'R' : 0, 'P' : 1, 'S' : 2, '' : 3}\n",
    "int2rps = {rps2int[c] : c for c in rps2int}\n",
    "rps_to_int = lambda hist : [rps2int[c] for c in hist]\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim = 3, output_dim = 8, mask_zero = False, batch_input_shape = (1, None)),    # converts character to trainable vectors. input_dim is size of vocabulary; output_dim is size of each vector; mask_zero is needed to deal with 0-padding in variable length input\n",
    "    tf.keras.layers.GRU(units = 64, return_sequences = False, stateful = True),    # units is dimensionality of output space\n",
    "    #tf.keras.layers.LSTM(units = 16),\n",
    "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
    "    #tf.keras.layers.Dense(8, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])\n",
    "model.compile(loss = tf.keras.losses.CategoricalCrossentropy(from_logits = True),    # this loss function works because it's applied across the last dimension of the predictions\n",
    "              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01))\n",
    "weights_0 = model.get_weights()\n",
    "\n",
    "def player(prev_play, opponent_history=[]):\n",
    "    opponent_history.append(prev_play)\n",
    "    if opponent_history[-1] == '':\n",
    "        tf.keras.backend.clear_session()\n",
    "        model.set_weights(weights_0)\n",
    "        model.reset_metrics()\n",
    "        model.reset_states()\n",
    "        opponent_history[:] = []\n",
    "    nhist = len(opponent_history)\n",
    "    if nhist > 1:\n",
    "        if len(opponent_history) == 499:\n",
    "            print(''.join(opponent_history))\n",
    "        n = 64\n",
    "        xs = [rps_to_int(opponent_history[i * n : (i+1) * n]) for i in range(len(opponent_history) // n)]\n",
    "        xs.append(rps_to_int(opponent_history[-n:]))\n",
    "        ys = [[0.0, 0.0, 0.0] for _ in xs]\n",
    "        for i in range(len(xs)):\n",
    "            xx = xs[i].pop()\n",
    "            ys[i][xx] = 1.0\n",
    "            dataset = tf.data.Dataset.from_tensors((np.array([xs[i]]), np.array([ys[i]])))\n",
    "            model.fit(dataset, epochs = 1, verbose = 0)\n",
    "        guess = rps[(1 + np.argmax(model.predict(np.array([xs[-1][1:] + [xx]]), verbose = 0))) % 3]\n",
    "    else:\n",
    "        guess = np.random.choice(rps)\n",
    "    return guess\n",
    "#play(player, quincy, 1000)\n",
    "play(player, abbey, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc11a42-82d4-40e3-a981-4b6c6332b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the best working approach. it uses both opponent's and self history. only recent history is used to make the next guess but for several epochs\n",
    "rps = ('R', 'P', 'S')\n",
    "rps2int = {'R' : 0, 'P' : 1, 'S' : 2, '' : 3}\n",
    "int2rps = {rps2int[c] : c for c in rps2int}\n",
    "rps_to_int = lambda hist : [rps2int[c] for c in hist]\n",
    "\n",
    "inp0 = tf.keras.Input(shape = (None,), batch_size = 1)\n",
    "x0 = tf.keras.layers.Embedding(input_dim = 3, output_dim = 2, mask_zero = False)(inp0)\n",
    "x0 = tf.keras.layers.GRU(units = 1, stateful = True, recurrent_initializer = 'glorot_uniform')(x0)\n",
    "inp1 = tf.keras.Input(shape = (None,), batch_size = 1)\n",
    "x1 = tf.keras.layers.Embedding(input_dim = 3, output_dim = 2, mask_zero = False)(inp1)\n",
    "x1 = tf.keras.layers.GRU(units = 1, stateful = True, recurrent_initializer = 'glorot_uniform')(x1)\n",
    "x2 = tf.keras.layers.concatenate([x0, x1])\n",
    "x2 = tf.keras.layers.Dense(units = 3, kernel_initializer = 'glorot_uniform')(x2)\n",
    "model = tf.keras.Model(inputs = (inp0, inp1), outputs = x2)\n",
    "model.compile(loss = tf.keras.losses.CategoricalCrossentropy(from_logits = True),    # this loss function works because it's applied across the last dimension of the predictions\n",
    "              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.005))\n",
    "weights_0 = model.get_weights()    # will be used to reset the model\n",
    "nn = enumerate([6, 3, 3, 11])    # how much history to pass while training for each player\n",
    "\n",
    "def player(prev_play, opponent_history=[], self_history=[], n0 = [6]):\n",
    "    opponent_history.append(prev_play)\n",
    "    # this resetting of model is done because main.py code keeps appending opponent history to this list but starts any new play with ''. this also allows to set any custom parameter for a specific player, like how n0 is used here\n",
    "    if opponent_history[-1] == '':\n",
    "        tf.keras.backend.clear_session()\n",
    "        model.set_weights(weights_0)\n",
    "        model.reset_metrics()\n",
    "        model.reset_states()\n",
    "        opponent_history[:] = []\n",
    "        self_history[:] = []\n",
    "        n0[0] = next(nn)[1]\n",
    "    nhist = len(opponent_history)\n",
    "    if nhist > 1:\n",
    "        if len(opponent_history) == 399:\n",
    "            print(''.join(opponent_history))\n",
    "            print(''.join(self_history))\n",
    "        n = n0[0]\n",
    "        x0 = rps_to_int(opponent_history[-min(nhist, n):])\n",
    "        x1 = rps_to_int(self_history[-min(nhist, n):])\n",
    "        y = [0.0, 0.0, 0.0]\n",
    "        y[x0[-1]] = 1.0\n",
    "        dataset = tf.data.Dataset.from_tensors(((np.array([x0[:-1]]), np.array([x1[:-1]])), np.array([y])))\n",
    "        model.fit(dataset, epochs = 8, verbose = 0)\n",
    "        guess = rps[(1 + np.argmax(model.predict((np.array([x0]), np.array([x1])), verbose = 0))) % 3]\n",
    "    else:\n",
    "        guess = np.random.choice(rps)\n",
    "    self_history.append(guess)\n",
    "    return guess\n",
    "#play(player, quincy, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf09cd-6404-46f9-b665-8fcc840e6b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This approach is hybrid of above 2 but doesn't work well enough\n",
    "rps = ('R', 'P', 'S')\n",
    "rps2int = {'R' : 0, 'P' : 1, 'S' : 2, '' : 3}\n",
    "int2rps = {rps2int[c] : c for c in rps2int}\n",
    "rps_to_int = lambda hist : [rps2int[c] for c in hist]\n",
    "\n",
    "inp0 = tf.keras.Input(shape = (None,), batch_size = 1)\n",
    "x0 = tf.keras.layers.Embedding(input_dim = 3, output_dim = 2, mask_zero = False)(inp0)\n",
    "x0 = tf.keras.layers.LSTM(units = 1, stateful = False, recurrent_initializer = 'glorot_uniform')(x0)\n",
    "inp1 = tf.keras.Input(shape = (None,), batch_size = 1)\n",
    "x1 = tf.keras.layers.Embedding(input_dim = 3, output_dim = 2, mask_zero = False)(inp1)\n",
    "x1 = tf.keras.layers.LSTM(units = 1, stateful = False, recurrent_initializer = 'glorot_uniform')(x1)\n",
    "x2 = tf.keras.layers.concatenate([x0, x1])\n",
    "x2 = tf.keras.layers.Dense(units = 3, kernel_initializer = 'glorot_uniform')(x2)\n",
    "model = tf.keras.Model(inputs = (inp0, inp1), outputs = x2)\n",
    "model.compile(loss = tf.keras.losses.CategoricalCrossentropy(from_logits = True),    # this loss function works because it's applied across the last dimension of the predictions\n",
    "              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.002))\n",
    "weights_0 = model.get_weights()\n",
    "nn = enumerate([6, 3, 3, 11])\n",
    "\n",
    "def player(prev_play, opponent_history=[], self_history=[], n0 = [6]):\n",
    "    opponent_history.append(prev_play)\n",
    "    if opponent_history[-1] == '':\n",
    "        tf.keras.backend.clear_session()\n",
    "        model.set_weights(weights_0)\n",
    "        model.reset_metrics()\n",
    "        model.reset_states()\n",
    "        opponent_history[:] = []\n",
    "        self_history[:] = []\n",
    "        n0[0] = next(nn)[1]\n",
    "    nhist = len(opponent_history)\n",
    "    if nhist > 1:\n",
    "        if len(opponent_history) == 399:\n",
    "            print(''.join(opponent_history))\n",
    "            print(''.join(self_history))\n",
    "        n = n0[0]\n",
    "        x0s = [rps_to_int(opponent_history[i * n : (i+1) * n]) for i in range(len(opponent_history) // n)]\n",
    "        x0s.append(rps_to_int(opponent_history[-n:]))\n",
    "        x1s = [rps_to_int(self_history[i * n : (i+1) * n]) for i in range(len(self_history) // n)]\n",
    "        x1s.append(rps_to_int(self_history[-n:]))\n",
    "        ys = [[0.0, 0.0, 0.0] for _ in x0s]\n",
    "        for i in range(len(x0s)):\n",
    "            xx = x0s[i].pop()\n",
    "            ys[i][xx] = 1.0\n",
    "            dataset = tf.data.Dataset.from_tensors(((np.array([x0s[i]]), np.array([x1s[i]])), np.array([ys[i]])))\n",
    "            model.fit(dataset, epochs = 1, verbose = 0)\n",
    "        guess = rps[(1 + np.argmax(model.predict((np.array([x0s[-1][1:] + [xx]]), np.array([x1s[-1]])), verbose = 0))) % 3]\n",
    "    else:\n",
    "        guess = np.random.choice(rps)\n",
    "    self_history.append(guess)\n",
    "    return guess\n",
    "#play(player, abbey, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582f755-8d06-4ed1-a2e3-99f6956302db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
